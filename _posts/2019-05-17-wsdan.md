---
layout: post
title: (论文)WS-DAN (弱监督数据增强)
date: 2019-05-16
tags: 深度学习/闪马
---
## 背景
* 近期在做外卖分类的项目，外卖分类也属于细粒度图像分类，在分类的过程中要从图片的行人中和非机动车中区分出各类外卖（主要是美团、饿了吗）。刚好近期发现了一片关于细粒度图像分类较新的论文（See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification），于是就准备亲手尝试一下。
* 论文地址：https://arxiv.org/pdf/1901.09891.pdf  [点击阅读](https://arxiv.org/pdf/1901.09891.pdf )

### 目录
* [Weakly Suprevised Learning(弱监督学习)](#Weakly Suprevised Learning)
* [Data Augmentation(数据增强)](#Data Augmentation)
* [WS-DAN (Weakly Supervised Data Augmentation Network)(弱监督数据增强网络)](#WS-DAN (Weakly Supervised Data Augmentation Network))

### <a name='Weakly Suprevised Learning'></a>Weakly Suprevised Learning(弱监督学习)
#### 什么是弱监督学习
弱监督是相对监督而言，所谓的监督简单的说就是label，所以监督的强弱就是从label来划分的，弱监督就是data的label并不是很完善的情况，其种类如下：
* 不完整监督： 部分样本label缺失。
即只有训练数据集的一个（通常很小的）子集有标签，其它数据则没有标签。在很多任务中都存在这种情况。例如，在图像分类中，真值标签是人工标注的；从互联网上获得大量的图片很容易，然而由于人工标注的费用，只能标注其中一个小子集的图像。
* 粗粒度监督： 只有粗粒度的标签。
又以图像分类任务为例。我们希望图片中的每个物体都被标注；然而我们只有图片级的标签而没有物体级的标签。比如说你有一张水果的图片，但是你不知道图片中的水果具体是苹果还是梨。
* 不准确监督：给的label包含噪声，甚至是错误的label，比如把“行人”标注为“汽车”。
即给定的标签并不总是真值。出现这种情况的原因有，标注者粗心或疲倦，或者一些图像本身就难以分类。
	
### <a name='Data Augmentation'></a>Data Augmentation(数据增强)
数据增强是常用的增加数据训练数据量的方法，被用来预防过拟合和提高深度学习模型的表现。在计算机视觉领域实践应用中常用的数据增强方法主要有：剪裁、翻转、旋转、比例缩放、位移、高斯噪声以及更高级的增强技术条件型生成对抗网络（Conditional GAN）。
* 剪裁(Crop)：从原始图像中随机抽样一部分，然后将此部分调整为原图像大小。这种方法通常也被称为`随机剪裁`。
* 翻转(Flip)：可以对图片进行水平和垂直翻转。
* 旋转(Rotation)：对图像按照图像中心进行旋转一定角度，并将大小作为原图的大小。
* 比例缩放(Scale)：图像可以向内或者向外缩放。向内缩放后通常图像会小于原图，通常会对超出边界做处内容假设；向外缩放后通常会大于原图，通常会新图中剪裁出一部分。（它和随机剪裁得到对图像具有一定区别，有兴趣可以自己拿一张图片试一下看一下效果）
* 位移(Translation)：对同图像中对目标按照x或y方向平移，因为多数情况，我们的目标对象可能出现在图像的任何位置。
* 高斯噪声(Gaussian Noise)：当神经网络试图学习高频特征（即非常频繁出现的无意义模式），而这些高频特征对模型提升没有什么帮助的时候发生过拟合(Overfitting)。因此，对这些数据人为加入噪声，使其特征失真，减弱其对模型的影响，`高斯噪声`就是这种方法之一，
* 条件型生成对抗网络(Conditional GANs)：是一种强大的神经网络，能将一张图片从一个领域转换到另一个领域中去，比如改变风景图片的季节、转换图片风格等。

#### 讲了那么多数据增强的方法，和弱监督学习有什么关系呢？
常用的数据增强方法中有很多采用的是随机的方法增广，像随机图像剪裁，对图像处理时是采用随机对目标图像进行剪裁对方法，这样做有一定的概率是能剪裁到我们需要对目标，但是更大的概率是会剪裁到我们并不需要的目标，比如背景等噪声目标。'(WS-DAN)Weakly Supervised Data Augmentation Network'（弱监督数据增强网络）在训练过程中通过弱监督学习产生一个用来表征目标显著特征的'attention map'；然后利用'attention map'有目标性的指导数据增强（包括注意力剪裁'attention cropping'和注意力丢弃'attention dropping'等）。

### <a name='WS-DAN (Weakly Supervised Data Augmentation Network)'></a>WS-DAN (Weakly Supervised Data Augmentation Network)(弱监督数据增强网络)
'WS-DAN'通过两种渠道增强图像分类的表现，一种方式：因为我们从图片中提取了显著的特征，使图片看起来更加有效'See Better'；另一种方式：注意力圈定了目标的位置，使得模型能够更‘近’的观察我们的目标从而提高模型表现'Look Closer'。
* `Attention Cropping`和`Attention Dropping`能够使模型做到`See Better Before Looking Closer`。`See Better`：`Attention map`表示了目标的显著特征部分。我们可以随机选择其中的一个显著特征部分将它删除，进而产生更加显著的特征部分；或者将闲着特征部分剪裁下来用于提取更加详细的特征。

<div align="center">
	<img src="/images/posts/WS-DAN/figure1.png" height="450" width="600">  
</div> 
* 训练过程：(a)弱监督注意力学习，通过弱监督注意力学习对每一张训练图片生成一个注意力图(attention maps)来表征对象对显著特征部分。（b）注意力引导数据增强，随机选择一张注意力图，通过注意力剪裁和注意力删除的方式去增强这张图片，最后原图和增强对数据都会被作为输入数据进行训练。

<div align="center">
	<img src="/images/posts/WS-DAN/fingure2-a.png" height="350" width="600">  
</div> 
* 测试过程：首先原图通过弱监督学习输出目标的类别概率和注意力图；然后通过目标定位和精炼定位目标的位置；最后将前两个阶段的数据结合。

<div align="center">
	<img src="/images/posts/WS-DAN/fingure2-b.png" height="350" width="600">  
</div> 
* BAP(Bilinear Attention Pooling)的过程：首先通过网络的主干（如resnet18）部分分别得到特征图（a）和注意力图（b）。每一个注意力图都代表了目标的特定部分。通过对注意力图和特征图的元素点乘得到各个分部特征图，让后利用卷积或者池化处理分部特征图，最后将各个分部特征图结合得到特征矩阵。

<div align="center">
	<img src="/images/posts/WS-DAN/fingure3.png" height="450" width="600">  
</div> 
#### 实践比较
* 注意力剪裁和随机剪裁的比较：
随机剪裁容易剪裁到图像的背景，而注意力剪裁知道取那些部分会see better。

<div align="center">
	<img src="/images/posts/WS-DAN/f4.png" height="250" width="600">  
</div> 
* 注意力丢弃和随机丢弃的比较：
随机丢弃可能会将整个目标丢弃或者只丢弃背景部分，而注意力丢弃对剔除目标显著部分和的注意力求具有更高的效率。

<div align="center">
	<img src="/images/posts/WS-DAN/f4-b.png" height="250" width="600">  
</div> 
#### 补充
* bilinear pooling:主要是外积，主要用于组合cnn（a和b）的feature map

<div align="center">
	<img src="/images/posts/WS-DAN/f5.png" height="250" width="600">  
</div> 

* 代码地址：https://github.com/GuYuc/WS-DAN.PyTorch  [点击跳转](https://github.com/GuYuc/WS-DAN.PyTorch )

